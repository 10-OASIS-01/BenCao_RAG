import os
import utils
import streamlit as st
from streaming import StreamHandler

from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import DocArrayInMemorySearch

st.set_page_config(page_title="æ–‡æ¡£å¢å¼ºåŒ»è¯é—®ç­”", page_icon="ğŸ“„")
st.header('ğŸ“„æ–‡æ¡£å¢å¼ºåŒ»è¯é—®ç­”')
st.write('æ”¯æŒè®¿é—®å’Œç”¨æˆ·ä¸Šä¼ çš„åŒ»ç–—è®°å½•ï¼ŒåŒ»å­¦æ–‡çŒ®ã€ä¸´åºŠæŒ‡å—ï¼Œæ ¹æ®å¼•ç”¨çš„ä¿¡æ¯ä¸ºç”¨æˆ·æŸ¥è¯¢æä¾›å‡†ç¡®çš„åŒ»å­¦ç­”æ¡ˆã€‚')

class CustomDataChatbot:

    def __init__(self):
        self.openai_model = utils.configure_openai()

    def save_file(self, file):
        folder = 'tmp'
        if not os.path.exists(folder):
            os.makedirs(folder)
        
        file_path = f'./{folder}/{file.name}'
        with open(file_path, 'wb') as f:
            f.write(file.getvalue())
        return file_path

    @st.spinner('Analyzing documents..')
    def setup_qa_chain(self, uploaded_files):
        # Load documents
        docs = []
        for file in uploaded_files:
            file_path = self.save_file(file)
            loader = PyPDFLoader(file_path)
            docs.extend(loader.load())
        
        # Split documents
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)

        # Create embeddings and store in vectordb
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
        vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)

        # Define retriever
        retriever = vectordb.as_retriever(
            search_type='mmr',
            search_kwargs={'k':2, 'fetch_k':4}
        )

        # Setup memory for contextual conversation        
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            return_messages=True
        )

        # Setup LLM and QA chain
        llm = ChatOpenAI(model_name=self.openai_model, temperature=0, streaming=True)
        qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory, verbose=True)
        return qa_chain

    @utils.enable_chat_history
    def main(self):

        # User Inputs
        uploaded_files = st.sidebar.file_uploader(label='è¯·ä¸Šä¼ æ‚¨çš„PDFæ–‡ä»¶', type=['pdf'], accept_multiple_files=True)
        if not uploaded_files:
            st.error("è¯·ä¸Šä¼ æ‚¨çš„PDFæ–‡ä»¶ä»¥ç»§ç»­")
            st.stop()

        user_query = st.chat_input(placeholder="æ‚¨å¥½ï¼Œæˆ‘æ˜¯æœ¬è‰RAGåŒ»è¯æ™ºèƒ½åŠ©ç†ï¼Œå¸Œæœ›å¯ä»¥å¸®åˆ°æ‚¨ã€‚ç¥æ‚¨èº«ä½“æ£’æ£’ï¼")

        if uploaded_files and user_query:
            qa_chain = self.setup_qa_chain(uploaded_files)

            utils.display_msg(user_query, 'user')

            with st.chat_message("assistant"):
                st_cb = StreamHandler(st.empty())
                result = qa_chain.invoke(
                    {"question":user_query},
                    {"callbacks": [st_cb]}
                )
                response = result["answer"]
                st.session_state.messages.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    obj = CustomDataChatbot()
    obj.main()